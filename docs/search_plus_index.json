{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Java-Spark系列 这是java的spark。 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 17:52:35 "},"chapter1/section1/":{"url":"chapter1/section1/","title":"1 当前大数据技术栈","keywords":"","body":"当前大数据技术栈 如图所示： 数据采集：一般通过Sqoop或Flume将关系型数据库数据同步到hadoop平台。 底层存储：采集到的数据存储在hdfs上，分布式进行存储。 资源调度：hadoop的资源调度就是yarn，用来协调各个集群节点的资源。 底层计算框架： 分MapReduce和Spark。 应用层：一般是BI自助分析 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 17:52:56 "},"chapter1/section2/":{"url":"chapter1/section2/","title":"2 spark 介绍","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 hadoop MapReduce框架局限性 2 spark 核心模块 3 Spark的好处 3.1 优势 3.2 特点 3.3 新特性 spark 介绍 1 hadoop MapReduce框架局限性 处理效率低效 1.1 Map 结果写磁盘，Reduce 写HDFS ，多个MR 之间通过HDFS 交换数据 ； 1.2 任务调度和启动开销大 ； 1.3 无法充分利用内存 不适合迭代计算（如机器学习 、 图计算等 ）， 交互式处理（数据挖掘 ） 不适合流式处理（点击日志分析 ） MapReduce编程不够灵活，仅支持Map 和Reduce 两种操作 因此需要一种灵活的框架可同时进行批处理、流式计算、交互式计算。 于是引入了Spark计算引擎。 2 spark 核心模块 Apache Spark是一个用于大规模数据处理的统一分析引擎。 它提供了Java、Scala、Python和R的高级api，以及支持通用执行图的优化引擎。 spark core：核心提供基础 spark sql：处理结构化数据操作 spark stream：处理增量计算和流式数据操作 spark mLib：机器学习相关 spark graphX：图形挖掘计算 3 Spark的好处 3.1 优势 内存计算引擎 ， 提供Cache 机制来支持需要反复迭代计算或者多次数据共享 ， 减少数据读取的IO 开销 DAG 引擎 ， 减少多次计算之间中间结果写到HDFS 的开销 使用多线程池模型来减少task 启动开稍 ，shuffle 过程中避免不必要的sort 操作以及减少磁盘IO 3.2 特点 易用：提供了丰富的API ， 支持Java ，Scala ，Python 和R 四种语言 R语言很少被用到，基本都是使用Java、Scala、Python来操作Spark 代码量比MapReduce 少2~5 倍 与Hadoop 集成 读写HDFS/Hbase 与YARN 集成 3.3 新特性 SparkSession：新的上下文入口，统一SQLContext和HiveContext dataframe与dataset统一，dataframe只是dataset[Row]的类型别名。由于Python是弱类型语言，只能使用DataFrame Spark SQL 支持sql 2003标准 支持ansi-sql 支持ddl命令 支持子查询：in/not in、exists/not exists 提升catalyst查询优化器的性能 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 18:03:47 "},"chapter1/extra/":{"url":"chapter1/extra/","title":"3 spark 常识","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 hadoop与spark的区别 2 核心模块 3 案例->wordCount spark 常识 1 hadoop与spark的区别 Spark是Hadoop MapReduce的替代者，而不是Hadoop的替代者。 hadoop：一次性数据计算和基于磁盘 框架在处理数据的时候，会从存储设备中读取数据，进行逻辑操作，然后将处理的结果重新存储到介质中。 上述图中显示为1个job，可以看见hadoop的数据处理模型很简单，但如果要处理复杂逻辑的时候，性能较低，不适合迭代处理，要把逻辑拆分成多个job，也就是上面图中的job多复制几个首尾相连。 spark：丰富的数据处理模型和基于内存来对数据集进行多次迭代。 对hadoop进行的升级，将中间的磁盘换成了内存，即把中间的计算结果放进内存中，为下一次的计算提供了更加快速的方式。 问题：spark如果部署在共享的环境中，可能会带来资源不足的问题。 2 核心模块 spark core：核心提供基础 spark sql：处理结构化数据操作 spark stream：处理流式数据操作 spark mLib：机器学习相关 spark graphX：图形挖掘计算 只学前3点。 3 案例->wordCount Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 17:46:40 "},"chapter2/":{"url":"chapter2/","title":"spark-java 运行流程","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 maven配置 2 java程序 3 打包到spark环境 spark-java 运行流程 1 maven配置 找java的spark。 org.apache.spark spark-sql_2.12 2.4.2 provided 2 java程序 计算在idcard.txt中包含' 1 '和包含' 2 '的行数。 package org.example; import org.apache.spark.api.java.function.FilterFunction; import org.apache.spark.sql.SparkSession; import org.apache.spark.sql.Dataset; public class SparkTest1 { public static void main(String[] args){ String logFile = \"file:///home/pyspark/idcard.txt\"; // Should be some file on your system SparkSession spark = SparkSession.builder().appName(\"Simple Application\").getOrCreate(); Dataset logData = spark.read().textFile(logFile).cache(); long numAs = logData.filter((FilterFunction) s -> s.contains(\"1\")).count(); long numBs = logData.filter((FilterFunction) s -> s.contains(\"2\")).count(); System.out.println(\"Lines with 1: \" + numAs + \", lines with 2: \" + numBs); spark.stop(); } } 3 打包到spark环境 mvn clean package到target目录下，将SparkStudy-1.0-SNAPSHOT.jar包复制到spark环境中，然后执行命令： spark-submit \\ --class org.example.SparkTest1 \\ --master local[2] \\ /home/javaspark/SparkStudy-1.0-SNAPSHOT.jar Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 18:16:36 "},"chapter3/section1/":{"url":"chapter3/section1/","title":"1 RDD 概念","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 特点 2 核心属性 2.1 分区列表 2.2 依赖列表 2.3 Compute函数 2.4 分区策略 2.5 优先位置列表 RDD 概念 RDD（resilient distributed dataset ，弹性分布式数据集），是 Spark 中最基础的抽象。 它表示了一个元素集合，其性质有： 并行操作的、 不可变的、 被分区了的。 用户不需要关心底层复杂的抽象处理，直接使用方便的算子处理和计算就可以了。 1 特点 分布式：RDD是一个抽象的概念，RDD在spark driver中，通过RDD来引用数据，数据真正存储在节点机的partition上。 只读：在Spark中RDD一旦生成了，就不能修改。 那么为什么要设置为只读，设置为只读的话，因为不存在修改，并发的吞吐量就上来了。 血缘关系：我们需要对RDD进行一系列的操作，因为RDD是只读的，我们只能不断的生产新的RDD，这样，新的RDD与原来的RDD就会存在一些血缘关系。 Spark会记录这些血缘关系，在后期的容错上会有很大的益处。 缓存 当一个 RDD 需要被重复使用时，或者当任务失败重新计算的时候，这时如果将 RDD 缓存起来，就可以避免重新计算，保证程序运行的性能。 RDD 的缓存有3种方式：cache、persist、checkPoint。 cache cache 方法不是在被调用的时候立即进行缓存，而是当触发了 action 类型的算子之后，才会进行缓存。 cache 和 persist 的区别：其实 cache 底层实际调用的就是 persist 方法，只是缓存的级别默认是 MEMORY_ONLY，而 persist 方法可以指定其他的缓存级别。 cache 和 checkPoint 的区别：checkPoint 是将数据缓存到本地或者 HDFS 文件存储系统中，当某个节点的 executor 宕机了之后，缓存的数据不会丢失，而通过 cache 缓存的数据就会丢掉。 checkPoint 的时候会把 job 从开始重新再计算一遍，因此在 checkPoint 之前最好先进行一步 cache 操作，cache 不需要重新计算，这样可以节省计算的时间。 persist 和 checkPoint 的区别：persist 也可以选择将数据缓存到磁盘当中，但是它交给 blockManager 管理的，一旦程序运行结束，blockManager 也会被停止，这时候缓存的数据就会被释放掉。而 checkPoint 持久化的数据并不会被释放，是一直存在的，可以被其它的程序所使用。 2 核心属性 RDD 调度和计算都依赖于5个属性。 2.1 分区列表 Spark RDD 是被分区的，每一个分区都会被一个计算任务 (Task) 处理，分区数决定了并行计算的数量，RDD 的并行度默认从父 RDD 传给子 RDD。 默认情况下，一个 HDFS 上的数据分片就是一个 partiton，RDD 分片数决定了并行计算的力度，可以在创建 RDD 时指定 RDD 分片个数， 如果不指定分区数量，当 RDD 从： 集合创建时，则默认分区数量为该程序所分配到的资源的 CPU 核数 (每个 Core 可以承载 2~4 个 partition)， HDFS 文件创建时，默认为文件的 Block 数。 2.2 依赖列表 RDD 是 Spark 的核心数据结构，由于 RDD 每次转换都会生成新的 RDD，所以 RDD 会形成类似流水线一样的前后依赖关系，通过对 RDD 的操作形成整个 Spark 程序。 RDD 之间的依赖有两种： 窄依赖 ( Narrow Dependency) 宽依赖 ( Wide Dependency) 当然宽依赖就不类似于流水线了，宽依赖后面的 RDD 具体的数据分片会依赖前面所有的 RDD 的所有数据分片，这个时候数据分片就不进行内存中的 Pipeline，一般都是跨机器的，因为有前后的依赖关系，所以当有分区的数据丢失时， Spark 会通过依赖关系进行重新计算，从而计算出丢失的数据，而不是对 RDD 所有的分区进行重新计算。 2.3 Compute函数 用于计算RDD各分区的值。 每个分区都会有计算函数， Spark 的 RDD 的计算函数是以分片为基本单位的，每个 RDD 都会实现 compute 函数，对具体的分片进行计算，RDD 中的分片是并行的，所以是分布式并行计算， 有一点非常重要，就是由于 RDD 有前后依赖关系，遇到宽依赖关系，如 reduce By Key 等这些操作时划分成 Stage， Stage 内部的操作都是通过 Pipeline 进行的，在具体处理数据时它会通过 Blockmanager 来获取相关的数据， 因为具体的 split 要从外界读数据，也要把具体的计算结果写入外界，所以用了一个管理器，具体的 split 都会映射成 BlockManager 的 Block，而体的 splt 会被函数处理，函数处理的具体形式是以任务的形式进行的。 2.4 分区策略 （可选） 每个 key-value 形式的 RDD 都有 Partitioner 属性，它决定了 RDD 如何分区。 当然，Partiton 的个数还决定了每个 Stage 的 Task 个数。 RDD 的分片函数可以分区 ( Partitioner)，可传入相关的参数，如 Hash Partitioner 和 Range Partitioner，它本身针对 key-value 的形式，如果不是 key-value 的形式它就不会有具体的 Partitioner， Partitioner 本身决定了下一步会产生多少并行的分片，同时它本身也决定了当前并行 ( Parallelize) Shuffle 输出的并行数据，从而使 Spark 具有能够控制数据在不同结点上分区的特性，用户可以自定义分区策略，如 Hash 分区等。 spark 提供了 partition By 运算符，能通过集群对 RDD 进行数据再分配来创建一个新的 RDD。 2.5 优先位置列表 （可选，HDFS实现数据本地化，避免数据移动） 优先位置列表会存储每个 Partition 的优先位置，对于一个 HDFS 文件来说，就是每个 Partition 块的位置。 观察运行 Spark 集群的控制台就会发现， Spark 在具体计算、具体分片以前，它已经清楚地知道任务发生在哪个结点上，也就是说任务本身是计算层面的、代码层面的，代码发生运算之前它就已经知道它要运算的数据在什么地方，有具体结点的信息。这就符合大数据中数据不动代码动的原则。 数据不动代码动的最高境界是数据就在当前结点的内存中。这时候有可能是 Memory 级别或 Tachyon 级别的， Spark 本身在进行任务调度时会尽可能地将任务分配到处理数据的数据块所在的具体位置。 据 Spark 的 RDD。 Scala 源代码函数 getParferredlocations 可知，每次计算都符合完美的数据本地性。 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 18:46:25 "},"chapter3/section2/":{"url":"chapter3/section2/","title":"2 RDD 数据集","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 maven 配置 2 初始化spark 2.1 RDD数据集 2.1.1 并行集合 2.1.2 外部数据集 2.1.3 外部数据集API RDD 数据集 1 maven 配置 org.apache.spark spark-core_2.12 2.4.2 provided org.apache.hadoop hadoop-client 2 初始化spark Spark程序必须做的第一件事是创建一个JavaSparkContext对象，它告诉Spark如何访问集群。 要创建SparkContext，首先需要构建包含应用程序信息的SparkConf对象。 import org.apache.spark.api.java.JavaSparkContext; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.SparkConf; SparkConf conf = new SparkConf().setAppName(appName).setMaster(master); JavaSparkContext sc = new JavaSparkContext(conf); 2.1 RDD数据集 Spark围绕弹性分布式数据集(RDD)的概念展开，RDD是一个可以并行操作的容错元素集合。 有2种创建rdd的方法: 在驱动程序中并行化一个现有的集合， 或者在外部存储系统中引用一个数据集，例如共享文件系统、HDFS、HBase或任何提供Hadoop InputFormat的数据源。 2.1.1 并行集合 通过在驱动程序中调用JavaSparkContext的parallelize方法可以创建并行集合。集合的元素被复制，形成一个可以并行操作的分布式数据集。 例如，下面是如何创建一个包含数字1到5的并行集合: List data = Arrays.asList(1, 2, 3, 4, 5); JavaRDD distData = sc.parallelize(data); 一旦创建了分布式数据集distData，就可以并行操作它了。例如，我们可以调用distData.reduce ((a, b) -> a + b)将列表中的元素相加。 稍后将描述对分布式数据集的操作。 并行集合的一个重要参数是将数据集分割成多个分区。Spark将为集群的每个分区运行一个任务。 通常，集群中的每个CPU需要2-4个分区。 通常，Spark会根据集群自动设置分区数量。但是，你也可以通过将它作为第二个参数传入parallelize来手动设置它(例如sc.parallelize(data, 10))。注意:代码中的某些地方使用术语片(分区的同义词)来保持向后兼容性。 2.1.2 外部数据集 Spark可以从Hadoop支持的任何存储源创建分布式数据集，包括： 本地文件系统、 HDFS、 Cassandra、 HBase、 Amazon S3等。 Spark支持文本文件、SequenceFiles和任何其他Hadoop InputFormat。 文本文件rdd可以使用SparkContext的textFile方法创建。该方法接受文件的URI(机器上的本地路径，或者hdfs://， s3a://等URI)，并将其作为行集合读取。下面是一个示例调用: JavaRDD distFile = sc.textFile(\"data.txt\"); 我们可以使用map和reduce操作将所有行的大小相加，如下所示:map(s - > s.length()).reduce((a, b) -> a + b) 2.1.3 外部数据集API Spark读取文件的一些注意事项: 如果使用本地文件系统上的路径，则必须在工作节点上的相同路径上访问该文件。 要么将文件复制到所有工作人员， 要么使用网络挂载的共享文件系统。 所有Spark基于文件的输入方法，包括textFile，都支持在目录、压缩文件和通配符上运行。 例如，可以使用 textFile(\"/my/directory\")、 textFile(\"/my/directory/.txt\") textFile(\"/my/directory/.gz\")。 textFile方法还接受第二个可选参数，用于控制文件的分区数。 默认情况下，Spark为文件的每个块创建一个分区(HDFS默认为128MB)，但你也可以通过传递一个更大的值来请求更多的分区。注意，分区不能少于块。 除了文本文件，Spark的Java API还支持其他几种数据格式: wholeTextFiles：JavaSparkContext.wholeTextFiles允许您读取包含多个小文本文件的目录，并以(文件名，内容)对的形式返回每个小文本文件。这与textFile相反，textFile在每个文件中每行返回一条记录。 SequenceFiles：使用SparkContext.sequenceFile[K, V]方法，其中K和V是文件中的键和值类型。这些应该是Hadoop的Writable接口的子类，比如IntWritable和Text。 其他Hadoop InputFormats：使用JavaSparkContext.hadoopRDD，它接受一个任意的JobConf和输入格式类、键类和值类。设置这些参数的方法与使用输入源设置Hadoop作业的方法相同。你也可以使用JavaSparkContext.newAPIHadoopRDD用于InputFormats，基于“新的”MapReduce API (org.apache.hadoop.mapreduce)。 JavaSparkContext.objectFile：JavaRDD.saveAsObjectFile，objectFile支持以由序列化的Java对象组成的简单格式保存RDD。虽然这种格式不如Avro这样的专用格式有效，但它提供了一种保存任何RDD的简单方法。 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-26 11:37:04 "},"chapter3/section3/":{"url":"chapter3/section3/","title":"3 RDD 操作","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 基础知识 2 将函数传递给Spark 3 理解闭包 3.1 foreach：Local vs. cluster modes 3.2 打印RDD的元素 4 使用键值对 5 常见Transformations操作及Actions操作 5.1 常见Transformations操作 5.2 常见Actions操作 5.3 总结 RDD 操作 rdd支持2种类型的操作: 转换：从现有数据集创建一个新的数据集 操作：在数据集上运行计算后向驱动程序返回一个值 例如， map是一种转换，它通过一个函数传递每个数据集元素并返回一个表示结果的新RDD。 reduce是一个操作，它使用一些函数聚合RDD的所有元素，并将最终结果返回给驱动程序(尽管也有一个并行的reduceByKey返回一个分布式数据集)。 Spark中的所有转换都是惰性的，因为它们不会立即计算结果。相反，他们只记住应用到一些基本数据集(例如，一个文件)的转换。只有当一个操作需要将结果返回给驱动程序时，才会计算转换。这种设计使Spark能够更高效地运行。 例如，我们可以认识到通过map创建的数据集将用于reduce，并且只将reduce的结果返回给驱动程序，而不是更大的映射数据集。 默认情况下，每次在转换后的RDD上运行操作时都可能重新计算。但是，您也可以使用persist(或cache)方法将RDD持久化到内存中，在这种情况下，Spark将在集群中保留元素，以便在下次查询时更快地访问它。它还支持在磁盘上持久化rdd，或跨多个节点复制rdd。 1 基础知识 为了说明RDD的基础知识，考虑下面这个简单的程序: //从外部文件定义了一个基本RDD。这个数据集不会被加载到内存中，也不会被执行，lines仅仅是一个指向文件的指针。 JavaRDD lines = sc.textFile(\"data.txt\"); //定义linelength作为映射转换的结果。同样，由于惰性，不能立即计算linelength。 JavaRDD lineLengths = lines.map(s -> s.length()); //运行reduce，这是一个动作。此时，Spark将计算分解为任务，在不同的机器上运行， //每台机器同时运行自己的部分map和局部reduce，只将其答案返回给驱动程序。 int totalLength = lineLengths.reduce((a, b) -> a + b); 如果我们以后还想使用lineLengths，我们可以添加下面这行代码在reduce之前: lineLengths.persist(StorageLevel.MEMORY_ONLY()); 这将导致linelength在第一次计算之后被保存在内存中。 2 将函数传递给Spark Spark的API在很大程度上依赖于在驱动程序中传递函数来在集群上运行。 在Java中，函数由实现org.apache.spark.api.java.function包中的接口的类表示。有2种方法可以创建这样的函数: 在您自己的类中实现Function接口，无论是匿名内部类还是命名内部类，并将其实例传递给Spark。 使用lambda表达式来精确定义一个实现。 本指南的大部分内容都使用lambda语法以达到简练性，但是也可以很轻松地以实现function这种长格式来使用所有相同的api。例如，我们可以这样写上面的代码: JavaRDD lines = sc.textFile(\"data.txt\"); JavaRDD lineLengths = lines.map(new Function() { public Integer call(String s) { return s.length(); } }); int totalLength = lineLengths.reduce(new Function2() { public Integer call(Integer a, Integer b) { return a + b; } }); 但极其不推荐下方这种内联函数写法： class GetLength implements Function { public Integer call(String s) { return s.length(); } } class Sum implements Function2 { public Integer call(Integer a, Integer b) { return a + b; } } JavaRDD lines = sc.textFile(\"data.txt\"); JavaRDD lineLengths = lines.map(new GetLength()); int totalLength = lineLengths.reduce(new Sum()); 3 理解闭包 Spark的难点之一是理解跨集群执行代码时变量和方法的作用域和生命周期。 在RDD范围之外修改变量的操作经常会引起混淆。在下面的示例中，我们将查看使用foreach()增加计数器的代码，但其他操作也可能出现类似的问题。 举例: 考虑下面的简单rdd.foreach，它的行为可能会因执行是否发生在同一个JVM中而不同。 一个常见的例子是： 在本地模式下运行Spark(如--master = local[n]) 与在集群中部署Spark应用(如Spark -submit to YARN) int counter = 0; JavaRDD rdd = sc.parallelize(data); //这样写是错的 rdd.foreach(x -> counter += x); println(\"Counter value: \" + counter); 上述代码的行为是未定义的，可能无法按照预期工作。 为了执行作业，Spark将RDD操作的处理分解为任务，每个任务由一个执行器执行。 在执行之前，Spark会计算任务的闭包。 闭包：是执行器在RDD上执行计算时必须可见的变量和方法(在本例中为foreach())。这个闭包被序列化并发送给每个执行器。 闭包中发送给每个执行器的变量现在是副本， 因此，当counter在foreach函数中被引用时，它不再是驱动程序节点上的计数器。驱动程序节点的内存中仍然有一个计数器，但对执行器不再可见！执行器只能看到序列化闭包的副本。 因此，counter的最终值仍然为零，因为计数器上的所有操作都引用了序列化闭包中的值。 3.1 foreach：Local vs. cluster modes 在本地模式下， 在某些情况下，foreach函数实际上将在与驱动程序相同的JVM中执行，并将引用相同的原始计数器，并可能实际更新它。 为了确保在这些场景中定义良好的行为，应该使用累加器。Spark中的累加器专门用于提供一种机制，当执行在集群中的工作节点之间拆分时，可以安全地更新变量。 本指南的“累加器”一节将对此进行更详细的讨论。 官方说法： 一般来说，闭包如循环或局部定义的方法这样的构造，不应该用来改变某些全局状态。 Spark不定义或保证从闭包外部引用的对象的突变行为。有些代码可能在本地模式下工作，但这只是偶然的，这样的代码在分布式模式下不会像预期的那样工作。如果需要一些全局聚合，则使用累加器。 3.2 打印RDD的元素 另一个常见的习惯用法是尝试使用RDD.foreach(println)或RDD.map(println)打印RDD的元素。 在一台机器上，这将生成预期的输出并打印所有RDD的元素。 而在集群模式下，执行器调用的到stdout的输出是现在正在写入执行器的stdout，而不是驱动程序上的stdout，所以驱动程序上的stdout不会显示这些！ 要打印驱动程序上的所有元素，可以使用collect()方法首先将RDD带到驱动程序节点:RDD.collect().foreach(println)。 但是，这可能会导致驱动程序耗尽内存，因为collect()将整个RDD获取到一台机器上； 如果你只需要打印RDD的一些元素，一个更安全的方法是使用take：RDD.take(100).foreach(println)。 4 使用键值对 虽然大多数Spark操作在包含任何类型对象的rdd上工作，但有一些特殊操作只在键值对的rdd上可用。最常见的是分布式“洗牌”操作，例如按键对元素进行分组或聚合。 在Java中，键值对使用scala表示，来自Scala标准库的Tuple2类。 可以简单地调用new Tuple2(a, b)来创建一个元组，然后使用tuple._1()和tuple._2()来访问它的key和value。 键值对的rdd由JavaPairRDD类表示。可以使用特殊版本的映射操作从javardd构建javapairrdd，如： mapToPair flatMapToPair JavaPairRDD既有标准的RDD函数，也有特殊的键值函数。 例如，以下代码使用键值对上的reduceByKey操作来计算每行文本在文件中出现的次数: JavaRDD lines = sc.textFile(\"data.txt\"); JavaPairRDD pairs = lines.mapToPair(s -> new Tuple2(s, 1)); JavaPairRDD counts = pairs.reduceByKey((a, b) -> a + b); 例如，我们也可以使用counts.sortByKey()来按字母顺序排序，最后使用counts.collect()将它们作为对象数组返回到驱动程序中。 注意：当在键值对操作中使用自定义对象作为键时，必须确保自定义equals()方法与匹配的hashCode()方法一起使用。有关详细信息，请参阅Object.hashCode()文档中概述的契约。 5 常见Transformations操作及Actions操作 5.1 常见Transformations操作 Transformation 含义 map(func) 对每个RDD元素应用func之后，构造成新的RDD filter(func) 对每个RDD元素应用func, 将func为true的元素构造成新的RDD flatMap(func) 和map类似,但是flatMap可以将一个输出元素映射成0个或多个元素。(也就是说func返回的是元素序列而不是单个元素). mapPartitions(func) 和map类似，但是在RDD的不同分区上独立执行。所以函数func的参数是一个Python迭代器，输出结果也应该是迭代器【即func作用为Iterator => Iterator】 mapPartitionsWithIndex(func) 和mapPartitions类似, but also provides func with an integer value representing the index of the partition, 但是还为函数func提供了一个正式参数，用来表示分区的编号。【此时func作用为(Int, Iterator) => Iterator 】 sample(withReplacement, fraction, seed) 抽样: fraction是抽样的比例0~1之间的浮点数; withRepacement表示是否有放回抽样, True是有放回, False是无放回；seed是随机种子。 union(otherDataset) 并集操作，重复元素会保留（可以通过distinct操作去重） intersection(otherDataset) 交集操作，结果不会包含重复元素 distinct([numTasks])) 去重操作 groupByKey([numTasks]) 把Key相同的数据放到一起【(K, V) => (K, Iterable)】，需要注意的问题：1. 如果分组(grouping)操作是为了后续的聚集(aggregation)操作(例如sum/average)， 使用reduceByKey或者aggregateByKey更高效。2.默认情况下，并发度取决于分区数量。我们可以传入参数numTasks来调整并发任务数。 reduceByKey(func, [numTasks]) 首先按Key分组，然后将相同Key对应的所有Value都执行func操作得到一个值。func必须是(V, V) => V'的计算操作。numTasks作用跟上面提到的groupByKey一样。 sortByKey([ascending], [numTasks]) 按Key排序。通过第一个参数True/False指定是升序还是降序。 join(otherDataset, [numTasks]) 类似SQL中的连接(内连接)，即(K, V) and (K, W) => (K, (V, W))，返回所有连接对。外连接通过:leftOUterJoin(左出现右无匹配为空)、rightOuterJoin（右全出现左无匹配为空）、fullOuterJoin实现（左右全出现无匹配为空）。 cogroup(otherDataset, [numTasks]) 对两个RDD做groupBy。即(K, V) and (K, W) => (K, Iterable, Iterable(W))。别名groupWith。 pipe(command, [envVars]) 将驱动程序中的RDD交给shell处理（外部进程），例如Perl或bash脚本。RDD元素作为标准输入传给脚本，脚本处理之后的标准输出会作为新的RDD返回给驱动程序。 coalesce(numPartitions) 将RDD的分区数减小到numPartitions。当数据集通过过滤减小规模时，使用这个操作可以提升性能。 repartition(numPartitions) 将数据重新随机分区为numPartitions个。这会导致整个RDD的数据在集群网络中洗牌。 repartitionAndSortWithinPartitions(partitioner) 使用partitioner函数充分去，并在分区内排序。这比先repartition然后在分区内sort高效，原因是这样迫使排序操作被移到了shuffle阶段。 5.2 常见Actions操作 Action 含义 reduce(func) 使用func函数聚集RDD中的元素（func接收两个参数返回一个值）。这个函数应该满足结合律和交换律以便能够正确并行计算。 collect() 将RDD转为数组返回给驱动程序。这个在执行filter等操作之后返回足够小的数据集是比较有用。 count() 返回RDD中的元素数量。 first() 返回RDD中的第一个元素。（同take(1)) take(n) 返回由RDD的前N个元素组成的数组。 takeSample(withReplacement, num, [seed]) 返回num个元素的数组，这些元素抽样自RDD，withReplacement表示是否有放回，seed是随机数生成器的种子）。 takeOrdered(n, [ordering]) 返回RDD的前Ｎ个元素，使用自然顺序或者通过ordering函数对将个元素转换为新的Key. saveAsTextFile(path) 将RDD元素写入文本文件。Spark自动调用元素的toString方法做字符串转换。 saveAsSequenceFile(path)(Java and Scala) 将RDD保存为Hadoop SequenceFile.这个过程机制如下：1. Pyrolite用来将序列化的Python RDD转为Java对象RDD;2. Java RDD中的Key/Value被转为Writable然后写到文件。 countByKey() 统计每个Key出现的次数，只对(K, V)类型的RDD有效，返回(K, int)词典。 foreach(func) 在所有RDD元素上执行函数func。 5.3 总结 上图是关于Transformations和Actions的一个介绍图解，可以发现， Transformations操作过后的RDD依旧是RDD， Actions过后的RDD都是非RDD。 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-26 12:47:10 "},"chapter3/section4/":{"url":"chapter3/section4/","title":"4 RDD 操作实例","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 初始化RDD 1.1 通过集合创建RDD 1.2 通过文件创建rdd 2 RDD的map操作 RDD 操作实例 1 初始化RDD 1.1 通过集合创建RDD java代码 import org.apache.spark.api.java.JavaSparkContext; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.SparkConf; import java.util.Arrays; import java.util.List; public class RDDTest1 { public static void main(String[] args){ SparkConf conf = new SparkConf().setAppName(\"RDDTest1\").setMaster(\"local[*]\"); JavaSparkContext sc = new JavaSparkContext(conf); //通过集合创建rdd List data = Arrays.asList(1, 2, 3, 4, 5); JavaRDD rdd = sc.parallelize(data); //查看list被分成了几部分 System.out.println(rdd.getNumPartitions() + \"\\n\"); //查看分区状况 System.out.println(rdd.glom().collect() + \"\\n\"); System.out.println(rdd.collect() + \"\\n\"); sc.stop(); } } spark环境运行 spark-submit \\ --class org.example.RDDTest1 \\ --master local[2] \\ /home/javaspark/SparkStudy-1.0-SNAPSHOT.jar 运行结果 4 [[1], [2], [3], [4, 5]] [1, 2, 3, 4, 5] 1.2 通过文件创建rdd 读取一个idcard.txt，获取年龄和性别。 java代码 import org.apache.spark.api.java.JavaSparkContext; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.SparkConf; import org.apache.spark.api.java.function.VoidFunction; import java.util.Calendar; public class RDDTest2 { public static void main(String[] args){ SparkConf conf = new SparkConf().setAppName(\"RDDTest2\").setMaster(\"local[2]\"); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD rdd1 = sc.textFile(\"file:///home/pyspark/idcard.txt\"); rdd1.foreach(new VoidFunction() { @Override public void call(String s) throws Exception { //System.out.println(s + \"\\n\"); Calendar cal = Calendar.getInstance(); int yearNow = cal.get(Calendar.YEAR); int monthNow = cal.get(Calendar.MONTH)+1; int dayNow = cal.get(Calendar.DATE); int year = Integer.valueOf(s.substring(6, 10)); int month = Integer.valueOf(s.substring(10,12)); int day = Integer.valueOf(s.substring(12,14)); String age; if ((month 运行结果 年龄: 65,性别:男; 年龄: 51,性别:男; 年龄: 52,性别:男; 年龄: 40,性别:男; 年龄: 42,性别:男; 年龄: 52,性别:男; 年龄: 52,性别:女; 年龄: 30,性别:男; 年龄: 45,性别:男; 年龄: 57,性别:女; 年龄: 41,性别:男; 2 RDD的map操作 求txt文档文本总长度 java代码 import org.apache.spark.api.java.JavaSparkContext; import org.apache.spark.api.java.JavaRDD; import org.apache.spark.SparkConf; import org.apache.spark.api.java.function.Function; import org.apache.spark.api.java.function.VoidFunction; public class RDDTest3 { public static void main(String[] args){ SparkConf conf = new SparkConf().setAppName(\"RDDTest3\").setMaster(\"local[2]\"); JavaSparkContext sc = new JavaSparkContext(conf); JavaRDD rdd1 = sc.textFile(\"file:///home/pyspark/idcard.txt\"); JavaRDD rdd2 = rdd1.map(s -> s.length()); //RDD使用函数：s.length()=>len(s) Integer total = rdd2.reduce((a, b) -> a + b); System.out.println(\"\\n\" + \"\\n\" + \"\\n\"); System.out.println(total); } public static int len(String s){ int str_length; str_length = s.length(); return str_length; } } 运行结果 [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18] 198 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-26 12:59:58 "},"chapter4/section1/":{"url":"chapter4/section1/","title":"1 spark 核心组件","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 Cluster Manager (Master,ResourceManager) 2 Worker (worker,NodeManager) 3 Driver 4 Executor 5 Application Spark 核心组件 SparkContext将资源需求提交给Cluster Manager， Cluster Manager根据需求分配资源； SparkContext将Task提交给Executor运行(Executor=Cache + Task)，Executor返回结果给SparkContext。 1 Cluster Manager (Master,ResourceManager) Spark的集群管理器，主要负责对整个集群资源的分配与管理。 Cluster Manager： 在 Yarn 部署模式下为 ResourceManager 在 Mesos 部署模式下为 Mesos Master 在 Standalone 部署模式下为 Master. 一般在Yarn部署的情况多一些。 2 Worker (worker,NodeManager) Spark 的工作节点。 在 Yarn 部署模式下实际由 NodeManager 替代. 主要负责以下工作: 将自己的内存, CPU 等资源通过注册机制告知 Cluster Manager 创建 Executor 将资源和任务进一步分配给 Executor 同步资源信息, Executor 状态信息给 ClusterManager 等 3 Driver Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。 Driver 在 Spark 作业执行时主要负责: 将用户程序转化为作业（Job） 在 Executor 之间调度任务（Task） 跟踪 Executor 的执行情况 通过 UI 展示查询运行情况 4 Executor Spark Executor 节点是负责在 Spark 作业中运行具体任务，任务彼此之间相互独立。 Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。 如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。 Executor 有2个核心功能： 负责运行组成 Spark 应用的任务，并将结果返回给驱动器（Driver） 它们通过自身的块管理器Block Manager为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。 5 Application 用户使用 Spark 提供的 API 编写的应用程序 Application 通过 Spark API 将进行 RDD 的转换和 DAG 的构建, 并通过 Driver 将 Application 注册到 Cluster Manager. Cluster Manager 将会根据 Application 的资源需求, 通过一级分配将 Executor, 内存, CPU 等资源分配给Application. Driver 通过二级分配将 Executor 等资源分配给每一个任务, Application 最后通过 Driver 告诉 Executor 运行任务 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-26 13:16:02 "},"chapter4/section2/":{"url":"chapter4/section2/","title":"2 spark on Yarn","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 Yarn的基本架构 2 Spark on Yarn 2.1 YARN-Cluster模式 2.2 YARN-Client模式 3 Spark Job Spark on Yarn 1 Yarn的基本架构 Yarn的3个组件: ResourceManager、 NodeManager、 Application Manager 2 Spark on Yarn 通过spark-submit命令可以运行spark的程序，如下所示: spark-submit --master MASTER_URL --deploy-mode DEPLOY_MODE 其中MASTER_URL有4种模式： Local: local/local[K]、local[*] Standalone:spark://HOSR:PORT Mesos:mesos://HOST:PORT YARN: yarn-client、yarn-cluster（根据本地hadoop配置） 调度pyspark程序实例: -- spark 2.4.0版本 spark-submit --master local xxxx.py spark-submit --master yarn --deploy-mode cluster xxxx.py spark-submit --master yarn --deploy-mode client xxxx.py 2.1 YARN-Cluster模式 资源申请、分配在Application Master完成， 任务执行计划、调度也在Application Master完成。 2.2 YARN-Client模式 资源申请、分配在Application Master完成， 任务执行计划、调度在Client端完成。 3 Spark Job SparkContext： 构建DAG图 将DAG图分解成Stage 把Taskset发送给Task Scheduler Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-26 13:25:13 "},"chapter5/section1/":{"url":"chapter5/section1/","title":"1 spark SQL 概述","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 概述 1.1 来源 1.2 Spark SQL 特点 1.3 Spark SQL的运行速度 2 Spark SQL数据抽象 2.1 DataFrame 2.2 Dataset 3 Spark SQL 操作数据库 3.1 Spark SQL操作Hive数据库 3.1.2 创建DataFrames 3.1.3 以编程方式运行SQL查询 3.2 Spark SQL操作MySQL数据库 Spark SQL 1 概述 1.1 来源 Hive是目前大数据领域，事实上的数据仓库标准。 Hive与RDBMS的SQL模型比较类似，容易掌握。 Hive的主要缺陷在于它的底层是基于MapReduce的，执行比较慢。 在Spark 0.x版的时候推出了Shark，Shark与Hive是紧密关联的，Shark底层很多东西还是依赖于Hive，修改了内存管理、物理计划、执行三个模块，底层使用Spark的基于内存的计算模型，性能上比Hive提升了很多倍。 在Spark 1.x的时候Shark被淘汰。在2014 年7月1日的Spark Summit 上， Databricks宣布终止对Shark的开发，将重点放到 Spark SQL 上。 Shark终止以后，产生了两个分支: Hive on Spark hive社区的，源码在hive中 Spark SQL(Spark on Hive) Spark社区，源码在Spark中，支持多种数据源，多种优化技术，扩展性好很多; Spark SQL的源码在Spark中，而且新增了许多的优化代码， 如果追求速度，例如数据分析的时候，可以使用Hive on Spark， 如果追求性能，例如生产的定时报表的时候，应该使用Spark SQL。 1.2 Spark SQL 特点 对比Spark RDD、Dataframe、Spark SQL代码实现wordcount: Spark RDD： sc.textFile(\"file:///home/hadoop/derby.log).flatMap(lambda x:x.split(\"\")).map(lambda x:(x,1)).reduceByKey(lambda a,b : a+b ).collect() Dataframe： df .groupBy('word' ).count( ).show() Spark SQL： ssc.sq|('select word,count(*) as cnt from s group by word') 可以看到，Spark SQL代码看起来与关系型数据库是一致的，并且发现Spark SQL的特点: 集成：通过Spark SQL或DataFrame API运行Spark 程序,操作更加简单、快速. 从上图可以看到，Spark SQL和DataFrame底层其实就是调用RDD 统一的数据访问：DataFrame 和SQL提供了访问各种数据源的通用方式，包括Hive、Avro、Parquet、ORC、JSON和JDBC。您甚至可以跨这些数据源连接数据。 Hive集成：在现有的数据仓库上运行SQL或HiveQL查询。 标准的连接：服务器模式为业务智能工具提供行业标准的JDBC和ODBC连接。 1.3 Spark SQL的运行速度 1). Python操作RDD比Java/Scala慢一倍以上 2). 无论是那种语言操作DataFrame，性能几乎一致 那么为什么Python用RDD这么慢? 为什么用Python写的RDD比Scala慢一倍以上？ 两种不同的语言的执行引擎，上下文切换、数据传输。 Spark SQL其实底层调用的也是RDD执行，其实中间的执行计划进行了优化，而且是在Spark的优化引擎里面,所以无论是那种语言操作DataFrame，性能几乎一致。 2 Spark SQL数据抽象 Spark SQL提供了两个新的抽象，分别是 Dataset和DataFrame； Dataset是数据的分布式集合。 Dataset是Spark 1.6中添加的一个新接口，它提供了RDDs的优点(强类型、使用强大lambda函数的能力)以及Spark SQL优化的执行引擎的优点。可以从JVM对象构造数据集，然后使用函数转换(map、flatMap、filter等)操作数据集。 数据集API可以在Scala和Java中使用。Python不支持Dataset API。 但是由于Python的动态特性，Dataset API的许多优点已经可以使用了(例如，您可以通过名称natural row. columnname访问行字段)。R的情况也是类似的。 DataFrame 是组织成命名列的Dataset。 它在概念上相当于关系数据库中的表或R/Python中的数据框架，但在底层有更丰富的优化。 数据框架可以从各种各样的数据源构建，例如:结构化数据文件、Hive中的表、外部数据库或现有的rdd。 DataFrame API可以在Scala、Java、Python和r中使用。在Scala和Java中，DataFrame是由行数据集表示的。 在Scala API中，DataFrame只是Dataset[Row]的类型别名。 在Java API中，用户需要使用Dataset来表示DataFrame。 2.1 DataFrame DataFrame的前身是SchemaRDD。Spark1.3更名为DataFrame。不继承RDD，自己实现RDD的大部分功能。 与RDD类似，DataFrame也是一个分布式数据集。 DataFrame可以看做分布式Row对象的集合，提供了由列组成的详细模式信息，使其可以得到优化，DataFrame不仅有比RDD更多的算子，还可以进行执行计划的优化 DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema DataFrame也支持嵌套数据类型（struct、array和Map） DataFrame API提供的是一套高层的关系操作，比函数式RDD API更加优化，门槛低 DataFrame的劣势在于在编译期缺少类型安全检查，导致运行时出错。 2.2 Dataset Dataset时在Spark1.6中添加的新接口；与RDD相比，可以保存更多的描述信息，概念上等同于关系型数据库中的二维表。与DataFrame相比，保存了类型信息，是强类型，提供了编译时检查。 调用Dataset的方法会生成逻辑计划，然后Spark的优化器进行优化，最终生成物理计划，然后提交到集群中运行。 Dataset包含了DataFrame的功能，在Spark2.0中两者得到了统一： DataFrame表示为Dataset[Row]，即Dataset的子集。 3 Spark SQL 操作数据库 3.1 Spark SQL操作Hive数据库 Spark中所有功能的入口点都是SparkSession类。 要创建一个基本的SparkSession，只需使用SparkSession.builder(): import org.apache.spark.sql.SparkSession; SparkSession spark = SparkSession .builder() .appName(\"Java Spark SQL basic example\") .config(\"spark.some.config.option\", \"some-value\") .getOrCreate(); 在Spark repo的examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java中可以找到完整的示例代码。 Spark 2.0中的SparkSession提供了对Hive特性的内置支持，包括使用HiveQL编写查询，访问Hive udf，以及从Hive表中读取数据的能力。要使用这些特性，您不需要有一个现有的Hive设置。 3.1.2 创建DataFrames 通过SparkSession，应用程序可以从现有的RDD、Hive表或Spark数据源中创建DataFrames。 下面是一个基于text文件内容的DataFrame示例: 代码： import org.apache.spark.sql.SparkSession; import org.apache.spark.sql.Dataset; import org.apache.spark.sql.Row; public class SparkSQLTest1 { public static void main(String[] args){ SparkSession spark = SparkSession .builder() .appName(\"SparkSQLTest1\") .config(\"spark.some.config.option\", \"some-value\") .getOrCreate(); Dataset df = spark.read().text(\"file:///home/pyspark/idcard.txt\"); df.show(); spark.stop(); } } 运行： spark-submit \\ --class org.example.SparkSQLTest1 \\ --master local[2] \\ /home/javaspark/SparkStudy-1.0-SNAPSHOT.jar 运行结果： +------------------+ | value| +------------------+ |440528*******63016| |350525*******60813| |120102*******10789| |452123*******30416| |440301*******22322| |441421*******54614| |440301*******55416| |232721*******40630| |362204*******88412| |430281*******91015| |420117*******88355| +------------------+ 3.1.3 以编程方式运行SQL查询 代码: import org.apache.spark.sql.SparkSession; import org.apache.spark.sql.Dataset; import org.apache.spark.sql.Row; public class SparkSQLTest2 { public static void main(String[] args){ SparkSession spark = SparkSession .builder() .appName(\"SparkSQLTest2\") .config(\"spark.some.config.option\", \"some-value\") .getOrCreate(); Dataset sqlDF = spark.sql(\"SELECT * FROM test.ods_fact_sale limit 100\"); sqlDF.show(); spark.stop(); } } 运行结果： +---+--------------------+---------+---------+ | id| sale_date|prod_name|sale_nums| +---+--------------------+---------+---------+ | 1|2011-08-16 00:00:...| PROD4| 28| | 2|2011-11-06 00:00:...| PROD6| 19| | 3|2011-04-25 00:00:...| PROD8| 29| | 4|2011-09-12 00:00:...| PROD2| 88| | 5|2011-05-15 00:00:...| PROD5| 76| | 6|2011-02-23 00:00:...| PROD6| 64| | 7|2012-09-26 00:00:...| PROD2| 38| | 8|2012-02-14 00:00:...| PROD6| 45| | 9|2010-04-22 00:00:...| PROD8| 57| | 10|2010-10-31 00:00:...| PROD5| 65| | 11|2010-10-24 00:00:...| PROD2| 33| | 12|2011-02-11 00:00:...| PROD9| 27| | 13|2012-07-10 00:00:...| PROD8| 48| | 14|2011-02-23 00:00:...| PROD6| 46| | 15|2010-08-10 00:00:...| PROD4| 50| | 16|2011-05-02 00:00:...| PROD6| 22| | 17|2012-07-20 00:00:...| PROD2| 56| | 18|2012-07-12 00:00:...| PROD9| 57| | 19|2011-11-18 00:00:...| PROD6| 58| | 20|2010-04-22 00:00:...| PROD4| 7| +---+--------------------+---------+---------+ only showing top 20 rows 3.2 Spark SQL操作MySQL数据库 Spark SQL不仅可以操作Hive数据库，也可以操作远程的MySQL数据库： import org.apache.spark.sql.SparkSession; import org.apache.spark.sql.Dataset; import org.apache.spark.sql.Row; public class SparkSQLTest3 { public static void main(String[] args){ SparkSession spark = SparkSession .builder() .appName(\"SparkSQLTest3\") .config(\"spark.some.config.option\", \"some-value\") .getOrCreate(); Dataset jdbcDF = spark.read() .format(\"jdbc\") .option(\"url\", \"jdbc:mysql://10.31.1.123:3306/test\") .option(\"dbtable\", \"(SELECT * FROM EMP) tmp\") .option(\"user\", \"root\") .option(\"password\", \"abc123\") .load(); jdbcDF.printSchema(); jdbcDF.show(); spark.stop(); } } 运行结果： root |-- empno: integer (nullable = true) |-- ename: string (nullable = true) |-- job: string (nullable = true) |-- mgr: integer (nullable = true) |-- hiredate: date (nullable = true) |-- sal: decimal(7,2) (nullable = true) |-- comm: decimal(7,2) (nullable = true) |-- deptno: integer (nullable = true) +-----+------+---------+----+----------+-------+-------+------+ |empno| ename| job| mgr| hiredate| sal| comm|deptno| +-----+------+---------+----+----------+-------+-------+------+ | 7369| SMITH| CLERK|7902|1980-12-17| 800.00| null| 20| | 7499| ALLEN| SALESMAN|7698|1981-02-20|1600.00| 300.00| 30| | 7521| WARD| SALESMAN|7698|1981-02-22|1250.00| 500.00| 30| | 7566| JONES| MANAGER|7839|1981-04-02|2975.00| null| 20| | 7654|MARTIN| SALESMAN|7698|1981-09-28|1250.00|1400.00| 30| | 7698| BLAKE| MANAGER|7839|1981-05-01|2850.00| null| 30| | 7782| CLARK| MANAGER|7839|1981-06-09|2450.00| null| 10| | 7788| SCOTT| ANALYST|7566|1987-06-13|3000.00| null| 20| | 7839| KING|PRESIDENT|null|1981-11-17|5000.00| null| 10| | 7844|TURNER| SALESMAN|7698|1981-09-08|1500.00| 0.00| 30| | 7876| ADAMS| CLERK|7788|1987-06-13|1100.00| null| 20| | 7900| JAMES| CLERK|7698|1981-12-03| 950.00| null| 30| | 7902| FORD| ANALYST|7566|1981-12-03|3000.00| null| 20| | 7934|MILLER| CLERK|7782|1982-01-23|1300.00| null| 10| +-----+------+---------+----+----------+-------+-------+------+ Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-26 14:53:13 "},"chapter5/section2/":{"url":"chapter5/section2/","title":"2 spark SQL编程实战","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 Spark DataFrame概述 1.1 创建DataFrame 1.1.1 通过json文件创建DataFrame 1.1.2 通过CSV文件创建DataFrame 1.1.3 通过hive table创建DataFrame 1.1.4 通过jdbc数据源创建DataFrame 2 Spark SQL实战 spark SQL编程实战 1 Spark DataFrame概述 在Spark语义中，DtatFrame是一个分布式的行集合，可以想象为一个关系型数据库的表，或一个带有列头的Excel表格。它和RDD一样，有这样一些特点: Immuatable: 一旦RDD、DataFrame被创建，就不能更改，只能通过tranformation生成新的RDD、DataFrame Lazy Evaluations: 只有action才会出发Transformation的执行。 Distributed: DataFrame和RDD一样都是分布式的。 1.1 创建DataFrame 支持的数据源: Parquet Files ORC Files JSON Files Hive Tables JDBC Avro Files 创建DataFrame的语法: Dataset df = spark.read().json(\"examples/src/main/resources/people.json\"); Spark SQL的起点: SparkSession 代码: import org.apache.spark.sql.SparkSession; SparkSession spark = SparkSession .builder() .appName(\"Java Spark SQL basic example\") .config(\"spark.some.config.option\", \"some-value\") .getOrCreate(); 使用SparkSession，应用程序可以从现有的RDD、Hive表或Spark数据源中创建DataFrames。 1.1.1 通过json文件创建DataFrame Json测试文件: {\"name\": \"Michael\", \"age\": 12} {\"name\": \"Andy\", \"age\": 13} {\"name\": \"Justin\", \"age\": 8} 代码: Dataset df = spark.read().json(\"file:///home/pyspark/test.json\"); 结果： +-----+-------+ |age | name | +-----+-------+ | 12 |Michael| | 13 | Andy | | 8 | Justin| +-----+-------+ 1.1.2 通过CSV文件创建DataFrame csv测试文件: 代码： Dataset df = spark.read().format(\"csv\").option(\"header\", \"true\").load(\"file:///home/pyspark/emp.csv\"); 结果： +-----+------+----+----------+ |empno| ename| mgr| hiredate| +-----+------+----+----------+ | 7369| SMITH|7902|1980-12-17| | 7499| ALLEN|7698|1981-02-20| | 7521| WARD|7698|1981-02-22| | 7566| JONES|7839|1981-04-02| | 7654|MARTIN|7698|1981-09-28| | 7698| BLAKE|7839|1981-05-01| | 7782| CLARK|7839|1981-06-09| | 7788| SCOTT|7566|1987-06-13| | 7839| KING|null|1981-11-17| | 7844|TURNER|7698|1981-09-08| | 7876| ADAMS|7788|1987-06-13| | 7900| JAMES|7698|1981-12-03| | 7902| FORD|7566|1981-12-03| | 7934|MILLER|7782|1982-01-23| +-----+------+----+----------+ 1.1.3 通过hive table创建DataFrame 代码： Dataset sqlDF = spark.sql(\"SELECT * FROM test.ods_fact_sale limit 100\"); 1.1.4 通过jdbc数据源创建DataFrame 代码： Dataset jdbcDF = spark.read() .format(\"jdbc\") .option(\"url\", \"jdbc:mysql://10.31.1.123:3306/test\") .option(\"dbtable\", \"(SELECT * FROM EMP) tmp\") .option(\"user\", \"root\") .option(\"password\", \"abc123\") .load(); jdbcDF.printSchema(); jdbcDF.show(); 2 Spark SQL实战 Java-Spark系列6-Spark SQL编程实战 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-26 15:15:05 "}}