{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Java-Spark系列 这是java的spark。 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 17:52:35 "},"chapter1/section1/":{"url":"chapter1/section1/","title":"当前大数据技术栈","keywords":"","body":"当前大数据技术栈 如图所示： 数据采集：一般通过Sqoop或Flume将关系型数据库数据同步到hadoop平台。 底层存储：采集到的数据存储在hdfs上，分布式进行存储。 资源调度：hadoop的资源调度就是yarn，用来协调各个集群节点的资源。 底层计算框架： 分MapReduce和Spark。 应用层：一般是BI自助分析 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 17:52:56 "},"chapter1/section2/":{"url":"chapter1/section2/","title":"spark 介绍","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 hadoop MapReduce框架局限性 2 spark 核心模块 3 Spark的好处 3.1 优势 3.2 特点 3.3 新特性 spark 介绍 1 hadoop MapReduce框架局限性 处理效率低效 1.1 Map 结果写磁盘，Reduce 写HDFS ，多个MR 之间通过HDFS 交换数据 ； 1.2 任务调度和启动开销大 ； 1.3 无法充分利用内存 不适合迭代计算（如机器学习 、 图计算等 ）， 交互式处理（数据挖掘 ） 不适合流式处理（点击日志分析 ） MapReduce编程不够灵活，仅支持Map 和Reduce 两种操作 因此需要一种灵活的框架可同时进行批处理、流式计算、交互式计算。 于是引入了Spark计算引擎。 2 spark 核心模块 Apache Spark是一个用于大规模数据处理的统一分析引擎。 它提供了Java、Scala、Python和R的高级api，以及支持通用执行图的优化引擎。 spark core：核心提供基础 spark sql：处理结构化数据操作 spark stream：处理增量计算和流式数据操作 spark mLib：机器学习相关 spark graphX：图形挖掘计算 3 Spark的好处 3.1 优势 内存计算引擎 ， 提供Cache 机制来支持需要反复迭代计算或者多次数据共享 ， 减少数据读取的IO 开销 DAG 引擎 ， 减少多次计算之间中间结果写到HDFS 的开销 使用多线程池模型来减少task 启动开稍 ，shuffle 过程中避免不必要的sort 操作以及减少磁盘IO 3.2 特点 易用：提供了丰富的API ， 支持Java ，Scala ，Python 和R 四种语言 R语言很少被用到，基本都是使用Java、Scala、Python来操作Spark 代码量比MapReduce 少2~5 倍 与Hadoop 集成 读写HDFS/Hbase 与YARN 集成 3.3 新特性 SparkSession：新的上下文入口，统一SQLContext和HiveContext dataframe与dataset统一，dataframe只是dataset[Row]的类型别名。由于Python是弱类型语言，只能使用DataFrame Spark SQL 支持sql 2003标准 支持ansi-sql 支持ddl命令 支持子查询：in/not in、exists/not exists 提升catalyst查询优化器的性能 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 18:03:47 "},"chapter1/extra/":{"url":"chapter1/extra/","title":"spark 常识","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 hadoop与spark的区别 2 核心模块 3 案例->wordCount spark 常识 1 hadoop与spark的区别 Spark是Hadoop MapReduce的替代者，而不是Hadoop的替代者。 hadoop：一次性数据计算和基于磁盘 框架在处理数据的时候，会从存储设备中读取数据，进行逻辑操作，然后将处理的结果重新存储到介质中。 上述图中显示为1个job，可以看见hadoop的数据处理模型很简单，但如果要处理复杂逻辑的时候，性能较低，不适合迭代处理，要把逻辑拆分成多个job，也就是上面图中的job多复制几个首尾相连。 spark：丰富的数据处理模型和基于内存来对数据集进行多次迭代。 对hadoop进行的升级，将中间的磁盘换成了内存，即把中间的计算结果放进内存中，为下一次的计算提供了更加快速的方式。 问题：spark如果部署在共享的环境中，可能会带来资源不足的问题。 2 核心模块 spark core：核心提供基础 spark sql：处理结构化数据操作 spark stream：处理流式数据操作 spark mLib：机器学习相关 spark graphX：图形挖掘计算 只学前3点。 3 案例->wordCount Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 17:46:40 "},"chapter2/":{"url":"chapter2/","title":"spark-java 运行流程","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 maven配置 2 java程序 3 打包到spark环境 spark-java 运行流程 1 maven配置 找java的spark。 org.apache.spark spark-sql_2.12 2.4.2 provided 2 java程序 计算在idcard.txt中包含' 1 '和包含' 2 '的行数。 package org.example; import org.apache.spark.api.java.function.FilterFunction; import org.apache.spark.sql.SparkSession; import org.apache.spark.sql.Dataset; public class SparkTest1 { public static void main(String[] args){ String logFile = \"file:///home/pyspark/idcard.txt\"; // Should be some file on your system SparkSession spark = SparkSession.builder().appName(\"Simple Application\").getOrCreate(); Dataset logData = spark.read().textFile(logFile).cache(); long numAs = logData.filter((FilterFunction) s -> s.contains(\"1\")).count(); long numBs = logData.filter((FilterFunction) s -> s.contains(\"2\")).count(); System.out.println(\"Lines with 1: \" + numAs + \", lines with 2: \" + numBs); spark.stop(); } } 3 打包到spark环境 mvn clean package到target目录下，将SparkStudy-1.0-SNAPSHOT.jar包复制到spark环境中，然后执行命令： spark-submit \\ --class org.example.SparkTest1 \\ --master local[2] \\ /home/javaspark/SparkStudy-1.0-SNAPSHOT.jar Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 18:16:36 "},"chapter3/section1/":{"url":"chapter3/section1/","title":"RDD 概念","keywords":"","body":"TreeviewCopyright © qgao 2021-* all right reserved, powered by aleen42 1 特点 2 核心属性 2.1 分区列表 2.2 依赖列表 2.3 Compute函数 2.4 分区策略 2.5 优先位置列表 RDD 概念 RDD（resilient distributed dataset ，弹性分布式数据集），是 Spark 中最基础的抽象。 它表示了一个元素集合，其性质有： 并行操作的、 不可变的、 被分区了的。 用户不需要关心底层复杂的抽象处理，直接使用方便的算子处理和计算就可以了。 1 特点 分布式：RDD是一个抽象的概念，RDD在spark driver中，通过RDD来引用数据，数据真正存储在节点机的partition上。 只读：在Spark中RDD一旦生成了，就不能修改。 那么为什么要设置为只读，设置为只读的话，因为不存在修改，并发的吞吐量就上来了。 血缘关系：我们需要对RDD进行一系列的操作，因为RDD是只读的，我们只能不断的生产新的RDD，这样，新的RDD与原来的RDD就会存在一些血缘关系。 Spark会记录这些血缘关系，在后期的容错上会有很大的益处。 缓存 当一个 RDD 需要被重复使用时，或者当任务失败重新计算的时候，这时如果将 RDD 缓存起来，就可以避免重新计算，保证程序运行的性能。 RDD 的缓存有3种方式：cache、persist、checkPoint。 cache cache 方法不是在被调用的时候立即进行缓存，而是当触发了 action 类型的算子之后，才会进行缓存。 cache 和 persist 的区别：其实 cache 底层实际调用的就是 persist 方法，只是缓存的级别默认是 MEMORY_ONLY，而 persist 方法可以指定其他的缓存级别。 cache 和 checkPoint 的区别：checkPoint 是将数据缓存到本地或者 HDFS 文件存储系统中，当某个节点的 executor 宕机了之后，缓存的数据不会丢失，而通过 cache 缓存的数据就会丢掉。 checkPoint 的时候会把 job 从开始重新再计算一遍，因此在 checkPoint 之前最好先进行一步 cache 操作，cache 不需要重新计算，这样可以节省计算的时间。 persist 和 checkPoint 的区别：persist 也可以选择将数据缓存到磁盘当中，但是它交给 blockManager 管理的，一旦程序运行结束，blockManager 也会被停止，这时候缓存的数据就会被释放掉。而 checkPoint 持久化的数据并不会被释放，是一直存在的，可以被其它的程序所使用。 2 核心属性 RDD 调度和计算都依赖于5个属性。 2.1 分区列表 Spark RDD 是被分区的，每一个分区都会被一个计算任务 (Task) 处理，分区数决定了并行计算的数量，RDD 的并行度默认从父 RDD 传给子 RDD。 默认情况下，一个 HDFS 上的数据分片就是一个 partiton，RDD 分片数决定了并行计算的力度，可以在创建 RDD 时指定 RDD 分片个数， 如果不指定分区数量，当 RDD 从： 集合创建时，则默认分区数量为该程序所分配到的资源的 CPU 核数 (每个 Core 可以承载 2~4 个 partition)， HDFS 文件创建时，默认为文件的 Block 数。 2.2 依赖列表 RDD 是 Spark 的核心数据结构，由于 RDD 每次转换都会生成新的 RDD，所以 RDD 会形成类似流水线一样的前后依赖关系，通过对 RDD 的操作形成整个 Spark 程序。 RDD 之间的依赖有两种： 窄依赖 ( Narrow Dependency) 宽依赖 ( Wide Dependency) 当然宽依赖就不类似于流水线了，宽依赖后面的 RDD 具体的数据分片会依赖前面所有的 RDD 的所有数据分片，这个时候数据分片就不进行内存中的 Pipeline，一般都是跨机器的，因为有前后的依赖关系，所以当有分区的数据丢失时， Spark 会通过依赖关系进行重新计算，从而计算出丢失的数据，而不是对 RDD 所有的分区进行重新计算。 2.3 Compute函数 用于计算RDD各分区的值。 每个分区都会有计算函数， Spark 的 RDD 的计算函数是以分片为基本单位的，每个 RDD 都会实现 compute 函数，对具体的分片进行计算，RDD 中的分片是并行的，所以是分布式并行计算， 有一点非常重要，就是由于 RDD 有前后依赖关系，遇到宽依赖关系，如 reduce By Key 等这些操作时划分成 Stage， Stage 内部的操作都是通过 Pipeline 进行的，在具体处理数据时它会通过 Blockmanager 来获取相关的数据， 因为具体的 split 要从外界读数据，也要把具体的计算结果写入外界，所以用了一个管理器，具体的 split 都会映射成 BlockManager 的 Block，而体的 splt 会被函数处理，函数处理的具体形式是以任务的形式进行的。 2.4 分区策略 （可选） 每个 key-value 形式的 RDD 都有 Partitioner 属性，它决定了 RDD 如何分区。 当然，Partiton 的个数还决定了每个 Stage 的 Task 个数。 RDD 的分片函数可以分区 ( Partitioner)，可传入相关的参数，如 Hash Partitioner 和 Range Partitioner，它本身针对 key-value 的形式，如果不是 key-value 的形式它就不会有具体的 Partitioner， Partitioner 本身决定了下一步会产生多少并行的分片，同时它本身也决定了当前并行 ( Parallelize) Shuffle 输出的并行数据，从而使 Spark 具有能够控制数据在不同结点上分区的特性，用户可以自定义分区策略，如 Hash 分区等。 spark 提供了 partition By 运算符，能通过集群对 RDD 进行数据再分配来创建一个新的 RDD。 2.5 优先位置列表 （可选，HDFS实现数据本地化，避免数据移动） 优先位置列表会存储每个 Partition 的优先位置，对于一个 HDFS 文件来说，就是每个 Partition 块的位置。 观察运行 Spark 集群的控制台就会发现， Spark 在具体计算、具体分片以前，它已经清楚地知道任务发生在哪个结点上，也就是说任务本身是计算层面的、代码层面的，代码发生运算之前它就已经知道它要运算的数据在什么地方，有具体结点的信息。这就符合大数据中数据不动代码动的原则。 数据不动代码动的最高境界是数据就在当前结点的内存中。这时候有可能是 Memory 级别或 Tachyon 级别的， Spark 本身在进行任务调度时会尽可能地将任务分配到处理数据的数据块所在的具体位置。 据 Spark 的 RDD。 Scala 源代码函数 getParferredlocations 可知，每次计算都符合完美的数据本地性。 Copyright © qgao 2021-* all right reserved，powered by Gitbook该文件修订时间： 2022-05-25 18:46:25 "}}